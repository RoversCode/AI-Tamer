{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import UNIMOLMHeadModel, UNIMOModel, UNIMOForMaskedLM\n",
    "from paddlenlp.transformers import UNIMOTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\softwares\\professional\\Anaconda3\\envs\\paddle2.4\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\softwares\\professional\\Anaconda3\\envs\\paddle2.4\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "\u001b[32m[2023-06-03 09:11:18,452] [    INFO]\u001b[0m - Model config UNIMOConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"hidden_act\": \"relu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"mask_token_id\": 3,\n",
      "  \"max_position_embeddings\": 513,\n",
      "  \"model_type\": \"unimo\",\n",
      "  \"normalize_before\": false,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"paddlenlp_version\": null,\n",
      "  \"type_vocab_size\": 4,\n",
      "  \"unk_token_id\": 17963,\n",
      "  \"vocab_size\": 18000\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[32m[2023-06-03 09:11:20,557] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing UNIMOLMHeadModel.\n",
      "\u001b[0m\n",
      "\u001b[32m[2023-06-03 09:11:20,558] [    INFO]\u001b[0m - All the weights of UNIMOLMHeadModel were initialized from the model checkpoint at unimo-text-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNIMOLMHeadModel for predictions without further training.\u001b[0m\n",
      "\u001b[32m[2023-06-03 09:11:20,559] [    INFO]\u001b[0m - Already cached C:\\Users\\SeungHee\\.paddlenlp\\models\\unimo-text-1.0\\unimo-text-1.0-vocab.txt\u001b[0m\n",
      "\u001b[32m[2023-06-03 09:11:20,576] [    INFO]\u001b[0m - tokenizer config file saved in C:\\Users\\SeungHee\\.paddlenlp\\models\\unimo-text-1.0\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-06-03 09:11:20,577] [    INFO]\u001b[0m - Special tokens file saved in C:\\Users\\SeungHee\\.paddlenlp\\models\\unimo-text-1.0\\special_tokens_map.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = UNIMOLMHeadModel.from_pretrained('unimo-text-1.0')\n",
    "tokenizer = UNIMOTokenizer.from_pretrained('unimo-text-1.0')\n",
    "\n",
    "inputs = tokenizer.gen_encode(\n",
    "    \"Welcome to use PaddlePaddle and PaddleNLP!\",\n",
    "    return_tensors=True,\n",
    "    is_split_into_words=False)\n",
    "logits = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-06-03 09:16:20,810] [    INFO]\u001b[0m - Model config UNIMOConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"hidden_act\": \"relu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"mask_token_id\": 3,\n",
      "  \"max_position_embeddings\": 513,\n",
      "  \"model_type\": \"unimo\",\n",
      "  \"normalize_before\": false,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"paddlenlp_version\": null,\n",
      "  \"type_vocab_size\": 4,\n",
      "  \"unk_token_id\": 17963,\n",
      "  \"vocab_size\": 18000\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[33m[2023-06-03 09:16:21,533] [ WARNING]\u001b[0m - Some weights of the model checkpoint at unimo-text-1.0 were not used when initializing UNIMOModel: ['lm_head.layer_norm.weight', 'lm_head.decoder_bias', 'lm_head.transform.weight', 'lm_head.transform.bias', 'lm_head.decoder_weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing UNIMOModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing UNIMOModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[32m[2023-06-03 09:16:21,533] [    INFO]\u001b[0m - All the weights of UNIMOModel were initialized from the model checkpoint at unimo-text-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNIMOModel for predictions without further training.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = UNIMOModel.from_pretrained('unimo-text-1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_example = tokenizer.gen_encode(\n",
    "    '早上好，今天空气质量。',\n",
    "    title=None,\n",
    "    target=None,\n",
    "    max_seq_len=512,\n",
    "    max_target_len=128,\n",
    "    max_title_len=30,\n",
    "    return_position_ids=True,\n",
    "    return_length=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 748, 28, 170, 4, 508, 125, 411, 266, 207, 150, 12043, 2],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'seq_len': 13,\n",
       " 'position_ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       " 'attention_mask': array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddlenlp.transformers import (\n",
    "    UNIMOLMHeadModel,\n",
    "    UNIMOTokenizer\n",
    ")\n",
    "\n",
    "paddle.seed(2)\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model_name_or_path = 'unimo-text-1.0'\n",
    "model = UNIMOLMHeadModel.from_pretrained(model_name_or_path)\n",
    "tokenizer = UNIMOTokenizer.from_pretrained(model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4] [1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Prepare the model inputs.\n",
    "inputs = tokenizer.gen_encode(\n",
    "    '早上好',\n",
    "    title=None,\n",
    "    target=None,\n",
    "    max_seq_len=512,\n",
    "    max_target_len=128,\n",
    "    max_title_len=30,\n",
    "    return_position_ids=True,\n",
    "    return_length=True,\n",
    "    return_tensors=True,\n",
    ")\n",
    "# Generate 2 sequences by using \"beam_search\" strategy (num_beams=5)\n",
    "ids, scores = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    token_type_ids=inputs['token_type_ids'],\n",
    "    position_ids=inputs['position_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    decode_strategy=\"beam_search\",\n",
    "    max_length=4,\n",
    "    min_length=2,\n",
    "    num_beams=5,\n",
    "    num_return_sequences=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(ids.shape, scores.shape)\n",
    "# [2, 3] [2, 1]\n",
    "# response = []\n",
    "# for sequence_ids in ids.numpy().tolist():\n",
    "#     sequence_ids = sequence_ids[:sequence_ids.index(tokenizer.sep_token_id)]\n",
    "#     text = tokenizer.convert_ids_to_string(sequence_ids, keep_space=False)\n",
    "#     response.append(text)\n",
    "# print(response)\n",
    "# # ['是的', '嗯嗯']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['。', '。', '。', '。', '。', '。', '。', '。', '。', '。']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(ids.numpy().tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paddle2.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
