{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "set_seed(55)\n",
    "generator = pipeline('text-generation', model='models\\pretrained_models\\WenZhong2.0-GPT2-3.5B-chinese',device=0)\n",
    "generator(\"我是一个外卖小哥，我的工作是\", max_length=100, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\Code\\ljj_person_project\\AI-Tamer\\NLP\\text_generation\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(r'..\\..\\pretrained_models\\nlp\\WenZhong2.0-GPT2-3.5B-chinese')\n",
    "model = GPT2LMHeadModel.from_pretrained(r'..\\..\\pretrained_models\\nlp\\WenZhong2.0-GPT2-3.5B-chinese').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是一个外卖小哥，我的工作是每天早上5点半起床，然后把外卖送到楼下，然后再把楼下的顾客送到楼上，每天工作时间是8个小时，每\n"
     ]
    }
   ],
   "source": [
    "# 从output预测中随机选择概率最大的前1个单词(这个前几个单词根据模型的性能来定，这个模型只能是1)\n",
    "def random_choice(preds, top_n=1):\n",
    "    x = random.choice(preds[0][-1,:].sort(descending=True)[1][:top_n])\n",
    "    return x.item()\n",
    "\n",
    "max_length = 100\n",
    "context = \"我是一个外卖小哥，我的工作是\"\n",
    "input_ids = tokenizer.encode(context)\n",
    "for _ in range(max_length):\n",
    "    output = model(torch.tensor(input_ids).to('cuda'))\n",
    "    predicted = random_choice(output)\n",
    "    if predicted == 50256:\n",
    "        print('遇到中止符')\n",
    "        break\n",
    "    input_ids = input_ids + [predicted]\n",
    "print(tokenizer.decode(input_ids))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '你好'\n",
    "input_ids = tokenizer(text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[19526,   254, 25001,   121]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50304, 3072)\n",
       "    (wpe): Embedding(1024, 3072)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-29): 30 x GPT2Block(\n",
       "        (ln_1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): FastGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flan-T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\softwares\\professional\\Anaconda3\\envs\\torch_cuda118\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# pip install accelerate\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(r\"../../pretrained_models/nlp/flan-t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(r\"../../pretrained_models/nlp/flan-t5-base\", device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13959,  1566,    12,  2968,    10,   571,   625,    33,    25,    58,\n",
      "             1]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\softwares\\professional\\Anaconda3\\envs\\torch_cuda118\\lib\\site-packages\\transformers\\generation\\utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Wie old sind Sie?</s>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"translate English to German: How old are you?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "print(input_ids)\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13959,  2830,    12,  1566,    10,     3,     2,     1]],\n",
      "       device='cuda:0')\n",
      "<pad> <unk></s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\softwares\\professional\\Anaconda3\\envs\\torch_cuda118\\lib\\site-packages\\transformers\\generation\\utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text = 'translate Chinese to English: 你好'\n",
    "input_ids = tokenizer(text, return_tensors='pt').input_ids.to(\"cuda\")\n",
    "print(input_ids)\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mt5-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MT5Tokenizer, MT5ForConditionalGeneration\n",
    "\n",
    "tokenizer = MT5Tokenizer.from_pretrained(r\"../../pretrained_models/nlp/mt5-base\")\n",
    "model = MT5ForConditionalGeneration.from_pretrained(r\"../../pretrained_models/nlp/mt5-base\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<extra_id_0>, a dog.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_text = \"summarize: studies have shown that owning a dog is good for you\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  259,  4235,  3586, 33332,     1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"你好啊\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  259,  4235,  3586, 33332,     1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好啊</s>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   259, 102690,    717, 131389,    894,   4965,   4851,    717,  57987,\n",
       "          12150,    894, 118181,    717, 198083,    894,   5579,  14952,    717,\n",
       "           8618,  11587,    894, 131389,   4851,    717, 157454,  87917, 131389,\n",
       "              1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "t = \"类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤\"\n",
    "x = tokenizer(t, return_tensors=\"pt\")\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['input_ids'].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8537, -1.0618, -0.4314, -1.7325, -1.1694],\n",
      "        [ 1.2923, -0.4685,  0.2874,  0.4401,  1.1569],\n",
      "        [ 0.3588, -0.7489,  0.5370, -0.8795,  0.2240]], requires_grad=True)\n",
      "tensor([1, 0, 4])\n",
      "tensor([[-1.5001, -1.7082, -1.0778, -2.3789, -1.8158],\n",
      "        [-1.0430, -2.8038, -2.0480, -1.8953, -1.1784],\n",
      "        [-1.3054, -2.4131, -1.1272, -2.5437, -1.4402]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[0.2231, 0.1812, 0.3403, 0.0927, 0.1627],\n",
      "        [0.3524, 0.0606, 0.1290, 0.1503, 0.3078],\n",
      "        [0.2711, 0.0895, 0.3239, 0.0786, 0.2369]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor(1.3971, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = nn.LogSoftmax(dim=1)\n",
    "k = nn.Softmax(dim=1)\n",
    "loss = nn.NLLLoss()\n",
    "# input is of size N x C = 3 x 5\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "print(input)\n",
    "# each element in target has to have 0 <= value < C  \n",
    "target = torch.tensor([1, 0, 4])\n",
    "print(target)\n",
    "print(m(input))\n",
    "print(k())\n",
    "output = loss(m(input), target) \n",
    "print(output) \n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\softwares\\professional\\Anaconda3\\envs\\torch_cuda118\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\SeungHee\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--bleu\\9e0985c1200e367cce45605ce0ecb5ede079894e0f24f54613fca08eeb8aff76 (last modified on Fri Jun 23 22:41:26 2023) since it couldn't be found locally at evaluate-metric--bleu, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "blue = evaluate.load('bleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = \"这款衬衫采用宽松的版型设计,穿着舒适又不挑身材。圆领设计,修饰颈部线条,修饰颈部线条。宽松的版型设计,穿着舒适又不挑身材。宽松的版型,遮肉显瘦。\"\n",
    "label = \"穿着舒适又不挑身材。圆领设计,修饰颈部线条,修饰颈部线条。宽松的版型设计,穿着舒适又不挑身材。宽松的版型,遮肉显瘦。\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.7860753021519787,\n",
       " 'precisions': [0.8181818181818182, 0.8, 0.7777777777777778, 0.75],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.2222222222222223,\n",
       " 'translation_length': 11,\n",
       " 'reference_length': 9}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blue.compute(predictions=[pred], references=[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['这',\n",
       " '款',\n",
       " '衬衫',\n",
       " '采用',\n",
       " '宽松',\n",
       " '的',\n",
       " '版型',\n",
       " '设计',\n",
       " ',',\n",
       " '穿着',\n",
       " '舒适',\n",
       " '又',\n",
       " '不',\n",
       " '挑',\n",
       " '身材',\n",
       " '。',\n",
       " '圆领',\n",
       " '设计',\n",
       " ',',\n",
       " '修饰',\n",
       " '颈部',\n",
       " '线条',\n",
       " ',',\n",
       " '修饰',\n",
       " '颈部',\n",
       " '线条',\n",
       " '。',\n",
       " '宽松',\n",
       " '的',\n",
       " '版型',\n",
       " '设计',\n",
       " ',',\n",
       " '穿着',\n",
       " '舒适',\n",
       " '又',\n",
       " '不',\n",
       " '挑',\n",
       " '身材',\n",
       " '。',\n",
       " '宽松',\n",
       " '的',\n",
       " '版型',\n",
       " ',',\n",
       " '遮肉',\n",
       " '显瘦',\n",
       " '。']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jieba import lcut\n",
    "\n",
    "lcut(\"这款衬衫采用宽松的版型设计,穿着舒适又不挑身材。圆领设计,修饰颈部线条,修饰颈部线条。宽松的版型设计,穿着舒适又不挑身材。宽松的版型,遮肉显瘦。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0, 'precisions': [0.31088082901554404, 0.047619047619047616, 0.010810810810810811, 0.0], 'brevity_penalty': 0.6338401097192047, 'length_ratio': 0.6868327402135231, 'translation_length': 193, 'reference_length': 281}\n"
     ]
    }
   ],
   "source": [
    "predictions = [\"这款衬衫采用宽松的版型设计,穿着舒适又不挑身材。圆领设计,修饰颈部线条,修饰颈部线条。宽松的版型设计,穿着舒适又不挑身材。宽松的版型,遮肉显瘦。\",\n",
    "               \"这款连衣裙选用优质的网纱面料制作而成,手感柔软舒适,穿着舒适透气,穿着舒适透气。高贵优雅的圆领设计,修饰颈部线条,修饰颈部线条。收腰的版型设计,修饰颈部线条,修饰颈部线条。\", \n",
    "               \"简约大方的阔腿裤,简约大方的直筒版型,修饰腿部线条,修饰腿部线条。宽松的阔腿裤型,修饰腿部线条,修饰腿部线条。宽松的阔腿裤型,修饰腿部线条。宽松的阔腿裤型,修饰腿部线条。\",\n",
    "               \"这款西装外套采用了蕾丝拼接的面料制作而成,穿着舒适透气,穿着舒适透气。拼接的蕾丝面料,穿着舒适透气。拼接的蕾丝面料,穿着舒适透气,穿着舒适。\"]\n",
    "references = [\n",
    "     \"萌趣印花点缀整体衣身，元气满满的衬衫让你在春天活力四射。气质的小半圆领设计，巧妙的勾勒你的脖颈线条，在视觉上增添高挑出众的气质。略微宽松的款式设计，巧妙的遮住你的肉肉，形成修长的线条美感，营造一身高级的慵懒感。不对称下摆的设计，增添了整体造型的层次感，巧妙的展现你独特的个性与魅力。\",\n",
    "     \"透露着清新淡雅，高贵知性，端庄的气质连衣裙，适合每个美眉。它经典的圆领设计，修饰了女性细长的脖颈。腰身处收腰的裁剪，不仅拉伸身材比例，更凸显了高挑的身姿。网纱刺绣的设计，把女性的高贵端庄，典雅知性气质展现的淋漓精致。\",\n",
    "     \"简约大方，时尚休闲，宽松直筒阔腿裤的版型设计简约却不简单，使得行走间自带清风，随性洒脱的魅力令人无法抵挡，同时，彰显出英姿飒爽的女王范儿。结合立体感的菱形提花面料，使得这条阔腿裤富有肌理感，低调而不失奢华地诠释着精致魅力。\",\n",
    "     \"春暖花开，万物复苏。又到了西装发挥作用的时候，西服的硬朗是不可磨灭的。这款袖口蕾丝的皮西装，中和了中性的感觉，在帅气与女人之间随意切换，肌理感pu皮面料。袖口与拼接蕾丝可脱卸，复古风十足~\",\n",
    "]\n",
    "\n",
    "results = blue.compute(predictions=predictions, references=references, tokenizer=lcut)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.0,\n",
       " 'precisions': [0.31088082901554404,\n",
       "  0.047619047619047616,\n",
       "  0.010810810810810811,\n",
       "  0.0],\n",
       " 'brevity_penalty': 0.6338401097192047,\n",
       " 'length_ratio': 0.6868327402135231,\n",
       " 'translation_length': 193,\n",
       " 'reference_length': 281}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这款衬衫采用宽松的版型设计,穿着舒适又不挑身材。圆领设计,修饰颈部线条,修饰颈部线条。宽松的版型设计,穿着舒适又不挑身材。宽松的版型,遮肉显瘦。\n",
      "萌趣印花点缀整体衣身，元气满满的衬衫让你在春天活力四射。气质的小半圆领设计，巧妙的勾勒你的脖颈线条，在视觉上增添高挑出众的气质。略微宽松的款式设计，巧妙的遮住你的肉肉，形成修长的线条美感，营造一身高级的慵懒感。不对称下摆的设计，增添了整体造型的层次感，巧妙的展现你独特的个性与魅力。\n",
      "{'bleu': 0.0, 'precisions': [0.34782608695652173, 0.044444444444444446, 0.0, 0.0], 'brevity_penalty': 0.4101204639912304, 'length_ratio': 0.5287356321839081, 'translation_length': 46, 'reference_length': 87}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for pre, label in zip(predictions, references):\n",
    "    print(pre)\n",
    "    print(label)\n",
    "    print(blue.compute(predictions=[pre], references=[label],tokenizer=lcut))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asdasdasdsa', 'asdas']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(\"asdasdasdsa asdas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Predictions and/or references don't match the expected format.\nExpected format:\nFeature option 0: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}\nFeature option 1: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')},\nInput predictions: ['这款', '衬衫'],\nInput references: ['这款', '衬衫']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m predictions \u001b[39m=\u001b[39m [ [\u001b[39m\"\u001b[39m\u001b[39m这款\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m衬衫\u001b[39m\u001b[39m\"\u001b[39m] ]\n\u001b[0;32m      2\u001b[0m references \u001b[39m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m      [\u001b[39m\"\u001b[39m\u001b[39m这款\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m衬衫\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      4\u001b[0m ]\n\u001b[1;32m----> 5\u001b[0m results \u001b[39m=\u001b[39m blue\u001b[39m.\u001b[39;49mcompute(predictions\u001b[39m=\u001b[39;49mpredictions, references\u001b[39m=\u001b[39;49mreferences)\n\u001b[0;32m      6\u001b[0m results\n",
      "File \u001b[1;32md:\\softwares\\professional\\Anaconda3\\envs\\torch_cuda118\\lib\\site-packages\\evaluate\\module.py:432\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m compute_kwargs \u001b[39m=\u001b[39m {k: kwargs[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m kwargs \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feature_names()}\n\u001b[0;32m    431\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m--> 432\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_batch(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[0;32m    433\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_finalize()\n\u001b[0;32m    435\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_file_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\softwares\\professional\\Anaconda3\\envs\\torch_cuda118\\lib\\site-packages\\evaluate\\module.py:480\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m batch \u001b[39m=\u001b[39m {input_name: batch[input_name] \u001b[39mfor\u001b[39;00m input_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feature_names()}\n\u001b[0;32m    479\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 480\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselected_feature_format \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infer_feature_from_batch(batch)\n\u001b[0;32m    481\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_writer()\n\u001b[0;32m    482\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\softwares\\professional\\Anaconda3\\envs\\torch_cuda118\\lib\\site-packages\\evaluate\\module.py:552\u001b[0m, in \u001b[0;36mEvaluationModule._infer_feature_from_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    551\u001b[0m     example \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m([(k, v[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems()])\n\u001b[1;32m--> 552\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infer_feature_from_example(example)\n",
      "File \u001b[1;32md:\\softwares\\professional\\Anaconda3\\envs\\torch_cuda118\\lib\\site-packages\\evaluate\\module.py:572\u001b[0m, in \u001b[0;36mEvaluationModule._infer_feature_from_example\u001b[1;34m(self, example)\u001b[0m\n\u001b[0;32m    565\u001b[0m feature_strings \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFeature option \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mfeature\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i, feature \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures)])\n\u001b[0;32m    566\u001b[0m error_msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    567\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPredictions and/or references don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match the expected format.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected format:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfeature_strings\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput predictions: \u001b[39m\u001b[39m{\u001b[39;00msummarize_if_long_list(example[\u001b[39m'\u001b[39m\u001b[39mpredictions\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput references: \u001b[39m\u001b[39m{\u001b[39;00msummarize_if_long_list(example[\u001b[39m'\u001b[39m\u001b[39mreferences\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    571\u001b[0m )\n\u001b[1;32m--> 572\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(error_msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Predictions and/or references don't match the expected format.\nExpected format:\nFeature option 0: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}\nFeature option 1: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')},\nInput predictions: ['这款', '衬衫'],\nInput references: ['这款', '衬衫']"
     ]
    }
   ],
   "source": [
    "predictions = [\"这款 衬衫 采用 宽松 的 版型 设计\"]\n",
    "references = [\n",
    "     \"这款 衬衫 采用 宽松 的 版型\"\n",
    "]\n",
    "results = blue.compute(predictions=predictions, references=references)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cuda118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
